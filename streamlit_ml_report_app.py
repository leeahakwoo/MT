
import streamlit as st
import pandas as pd
import joblib
import matplotlib.pyplot as plt
import seab as sns
from sklearn.metrics import confusion_matrix, roc_curve, auc
from fpdf import FPDF
import tempfile
import os

def generate_formula_image_fixed():
    fig, ax = plt.subplots(figsize=(10, 2))
    ax.axis("off")
    formula_text = (
        r"$\mathrm{Precision} = \frac{TP}{TP + FP} \quad "
        r"\mathrm{Recall} = \frac{TP}{TP + FN} \quad "
        r"F1 = 2 \cdot \frac{\mathrm{Precision} \cdot \mathrm{Recall}}{\mathrm{Precision} + \mathrm{Recall}}$"
    )
    ax.text(0.5, 0.5, formula_text, fontsize=16, ha="center", va="center")
    formula_path = "formula_fixed.png"
    plt.savefig(formula_path, bbox_inches="tight", dpi=200)
    plt.close()
    return formula_path

st.title("ğŸ§  Model Evaluation with All Upgrades (1-4)")

uploaded_model = st.file_uploader("Upload trained model (.pkl or .joblib)", type=["pkl", "joblib"])
uploaded_test_data = st.file_uploader("Upload test data (.csv)", type=["csv"])

def plot_confusion_matrix(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    fig, ax = plt.subplots()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax)
    ax.set_title("Confusion Matrix")
    return fig, cm

def plot_roc(y_true, y_score):
    fpr, tpr, _ = roc_curve(y_true, y_score)
    roc_auc = auc(fpr, tpr)
    fig, ax = plt.subplots()
    ax.plot(fpr, tpr, label=f"AUC = {roc_auc:.2f}")
    ax.plot([0, 1], [0, 1], 'k--')
    ax.set_title("ROC Curve")
    ax.set_xlabel("False Positive Rate")
    ax.set_ylabel("True Positive Rate")
    ax.legend(loc="lower right")
    return fig, roc_auc

def detailed_metric_table(TP, FP, FN):
    try:
        precision = TP / (TP + FP)
        recall = TP / (TP + FN)
        f1 = 2 * precision * recall / (precision + recall)
    except ZeroDivisionError:
        precision = recall = f1 = 0
    table_data = {
        "í•­ëª©": ["Precision", "Recall", "F1 Score"],
        "ê°’": [round(precision, 3), round(recall, 3), round(f1, 3)],
        "ê³„ì‚°ì‹": [
            f"{TP} / ({TP} + {FP})",
            f"{TP} / ({TP} + {FN})",
            f"2 * {round(precision, 3)} * {round(recall, 3)} / ({round(precision, 3)} + {round(recall, 3)})"
        ]
    }
    return pd.DataFrame(table_data), precision, recall, f1

def generate_pdf_table(df, pdf):
    pdf.set_font("Arial", "B", 12)
    pdf.cell(0, 10, "Metric Summary Table:", ln=True)
    pdf.set_font("Arial", "", 11)
    col_width = pdf.w / 3.5
    row_height = 8
    pdf.cell(col_width, row_height, "í•­ëª©", border=1)
    pdf.cell(col_width, row_height, "ê°’", border=1)
    pdf.cell(col_width*2, row_height, "ê³„ì‚°ì‹", border=1)
    pdf.ln(row_height)
    for index, row in df.iterrows():
        pdf.cell(col_width, row_height, str(row["í•­ëª©"]), border=1)
        pdf.cell(col_width, row_height, str(row["ê°’"]), border=1)
        pdf.cell(col_width*2, row_height, str(row["ê³„ì‚°ì‹"]), border=1)
        pdf.ln(row_height)

def generate_pdf(metrics_df, process_lines, explanations, chart_paths, formula_image, viz_explanations):
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    pdf.cell(0, 10, "Model Evaluation Report (Detailed)", ln=True)

    if os.path.exists(formula_image):
        pdf.image(formula_image, w=180)

    pdf.set_font("Arial", "B", 12)
    pdf.cell(0, 10, "Test Process Summary:", ln=True)
    pdf.set_font("Arial", "", 11)
    for line in process_lines:
        pdf.multi_cell(0, 8, line)

    generate_pdf_table(metrics_df, pdf)

    pdf.ln(5)
    pdf.set_font("Arial", "B", 12)
    pdf.cell(0, 10, "Interpretation:", ln=True)
    pdf.set_font("Arial", "", 11)
    for explanation in explanations:
        pdf.multi_cell(0, 8, explanation)

    for path, desc in chart_paths:
        pdf.add_page()
        pdf.image(path, w=180)
        pdf.ln(2)
        pdf.set_font("Arial", "I", 10)
        pdf.multi_cell(0, 8, desc)

    temp_path = tempfile.mktemp(suffix=".pdf")
    pdf.output(temp_path)
    return temp_path

if uploaded_model and uploaded_test_data:
    try:
        model = joblib.load(uploaded_model)
        df = pd.read_csv(uploaded_test_data)

        if 'target' not in df.columns:
            st.error("'target' column is required in the test dataset.")
        else:
            X_test = df.drop(columns=['target'])
            y_test = df['target']
            y_pred = model.predict(X_test)

            fig_cm, cm = plot_confusion_matrix(y_test, y_pred)
            st.subheader("ğŸ“Œ Confusion Matrix")
            st.pyplot(fig_cm)
            st.markdown("*ğŸ” ëŒ€ê°ì„  ê°’ì´ ë†’ì„ìˆ˜ë¡ ì˜¬ë°”ë¥¸ ì˜ˆì¸¡ ë¹„ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤. FPì™€ FNì€ ì˜¤ì˜ˆì¸¡ì…ë‹ˆë‹¤.*")

            TP = cm[1][1]
            FP = cm[0][1]
            FN = cm[1][0]
            TN = cm[0][0]

            process_lines = [
                f"TP = {TP}, FP = {FP}, FN = {FN}, TN = {TN}"
            ]
            metrics_df, precision, recall, f1 = detailed_metric_table(TP, FP, FN)

            explanations = [
                f"Precision: {precision:.2f} â†’ ì˜ˆì¸¡ì´ ë§ì„ í™•ë¥ ",
                f"Recall: {recall:.2f} â†’ ì‹¤ì œ ì •ë‹µì„ ë†“ì¹˜ì§€ ì•Šì„ í™•ë¥ ",
                f"F1 Score: {f1:.2f} â†’ Precisionê³¼ Recallì˜ ì¡°í™” í‰ê· "
            ]

            st.subheader("ğŸ“Š Metric Table")
            st.dataframe(metrics_df)

            chart_paths = []
            cm_path = tempfile.mktemp(suffix=".png")
            fig_cm.savefig(cm_path)
            chart_paths.append((cm_path, "ğŸ” Confusion MatrixëŠ” ì˜ˆì¸¡ vs ì‹¤ì œë¥¼ ë¹„êµí•˜ëŠ” í‘œì…ë‹ˆë‹¤."))

            try:
                y_score = model.predict_proba(X_test)[:, 1]
                fig_roc, roc_auc = plot_roc(y_test, y_score)
                st.subheader("ğŸ“ˆ ROC Curve")
                st.pyplot(fig_roc)
                st.markdown(f"*ğŸ¯ AUC = {roc_auc:.2f}. ë†’ì„ìˆ˜ë¡ êµ¬ë¶„ ëŠ¥ë ¥ì´ ì¢‹ìŠµë‹ˆë‹¤.*")
                roc_path = tempfile.mktemp(suffix=".png")
                fig_roc.savefig(roc_path)
                chart_paths.append((roc_path, f"ğŸ¯ ROC Curve: AUC = {roc_auc:.2f}. ë†’ì€ AUCëŠ” ìš°ìˆ˜í•œ ë¶„ë¥˜ê¸°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤."))
                plt.close(fig_roc)
            except Exception as e:
                chart_paths.append(("", f"[!] ROC Curve ì˜ˆì™¸ ë°œìƒ: {e}"))

            formula_path = generate_formula_image_fixed()
            st.image(formula_path, caption="ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ìˆ˜ì‹")

            # Explainability (fallback without SHAP)
            st.subheader("ğŸ§  ì˜ˆì¸¡ ì„¤ëª… ëŒ€ì•ˆ")
            if hasattr(model, "feature_importances_"):
                importances = model.feature_importances_
                feat_df = pd.DataFrame({
                    "Feature": X_test.columns,
                    "Importance": importances
                }).sort_values(by="Importance", ascending=False)
                st.bar_chart(feat_df.set_index("Feature"))
                st.markdown("*ì´ ëª¨ë¸ì€ SHAP ì—†ì´ feature_importances_ë¡œ ì˜ˆì¸¡ ê·¼ê±°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.*")
            else:
                st.markdown("ì´ ëª¨ë¸ì€ feature importance ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

            if st.button("ğŸ“„ Generate Detailed PDF Report"):
                pdf_path = generate_pdf(
                    metrics_df=metrics_df,
                    process_lines=process_lines,
                    explanations=explanations,
                    chart_paths=chart_paths,
                    formula_image=formula_path,
                    viz_explanations=[desc for _, desc in chart_paths if desc]
                )
                with open(pdf_path, "rb") as f:
                    st.download_button("ğŸ“¥ Download Report", f, file_name="detailed_evaluation_report.pdf")

    except Exception as e:
        st.error(f"[ERROR] ëª¨ë¸ í‰ê°€ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
